{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make one big unified datasets from a group of already cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "from PIL import Image\n",
    "import os, shutil\n",
    "import yaml\n",
    "\n",
    "def RecreateDatasetFolder(RootPaths, *args):\n",
    "    # If alreaty exixst remove the directory and all its files to create a fresh new dataset from scratch\n",
    "    if os.path.exists(RootPaths):\n",
    "        print(\"Removing Old Images Files and folders\")\n",
    "        shutil.rmtree(RootPaths)\n",
    "        print(\"Finished\")\n",
    "\n",
    "    # Create new images and labels folders structure\n",
    "    print(\"Creating New Images Files and folders\")\n",
    "    for path in args:\n",
    "        os.makedirs(path)\n",
    "    print(\"Creation Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Path declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Dataset name for the new dataset we want to create\n",
    "NEW_DATASET_NAME = \"GigaDataset\"\n",
    "\n",
    "# Source directory that contains all the datasets we want to merge in the YOLOv8 format\n",
    "SRC_DATASETS_PATH = \"./CleanedDatasets\"\n",
    "# Destination Directory for the new created dataset\n",
    "DST_DATASET_PATH = f\"./{NEW_DATASET_NAME}\"\n",
    "\n",
    "DST_DATASET_IMAGES_PATH = os.path.join(DST_DATASET_PATH, \"images\")\n",
    "DST_DATASET_LABELS_PATH = os.path.join(DST_DATASET_PATH, \"labels\")\n",
    "\n",
    "# Path for discarded images because of they have zero Labels\n",
    "DST_DATASET_DISCARDED_IMAGES_NO_LABELS_PATH = os.path.join(\"discarded\", \"noLabels\", \"images\")\n",
    "DST_DATASET_DISCARDED_LABELS_NO_LABELS_PATH = os.path.join(\"discarded\", \"noLabels\", \"labels\")\n",
    "\n",
    "# Path for discarded images because of they have a very small Bounding Box\n",
    "DST_DATASET_DISCARDED_IMAGES_TOO_SMALL_PATH = os.path.join(\"discarded\", \"tooSmall\", \"images\")\n",
    "DST_DATASET_DISCARDED_LABELS_TOO_SMALL_PATH = os.path.join(\"discarded\", \"tooSmall\", \"labels\")\n",
    "\n",
    "# Percentage of area covered by the bounding box\n",
    "BB_COVERAGE_THRESHOLD = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discardBBdimension(imagePath, labelPath) -> bool:\n",
    "    # Get the image resolution\n",
    "    with Image.open(imagePath) as image:\n",
    "        width, height = image.size\n",
    "\n",
    "    with open(labelPath, \"r\") as label_fin:\n",
    "        # Counter of the BB founded in the Image\n",
    "        bb_counter = 0\n",
    "        # List to store the BB, in the image, that are under the threshold \n",
    "        bb_eval = []\n",
    "\n",
    "        for line in label_fin:\n",
    "            bb_counter += 1\n",
    "            data = line.split(\" \")\n",
    "\n",
    "            # Rescale the normalized BB values\n",
    "            dim_x, dim_y = width * float(data[3]), height * float(data[4])\n",
    "            \n",
    "            bb_eval.append((dim_x * dim_y) / (width * height) * 100 < BB_COVERAGE_THRESHOLD) \n",
    "\n",
    "    # If the BBs under the threshold are more than the 50% of the total BB the image is discarded\n",
    "    return sum(bb_eval) / bb_counter * 100 >= 50\n",
    "\n",
    "\n",
    "def isLabelFileEmpty(filePath) -> bool:\n",
    "    return os.path.getsize(filePath) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets and remove images with no label in label file and with small Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each time we run this script we erase the previously created stuff to restart from a clean environment\n",
    "# Comment this line if needed\n",
    "RecreateDatasetFolder(DST_DATASET_PATH, \n",
    "                      DST_DATASET_IMAGES_PATH, \n",
    "                      DST_DATASET_LABELS_PATH, \n",
    "                      DST_DATASET_DISCARDED_IMAGES_NO_LABELS_PATH,\n",
    "                      DST_DATASET_DISCARDED_LABELS_NO_LABELS_PATH,\n",
    "                      DST_DATASET_DISCARDED_IMAGES_TOO_SMALL_PATH,\n",
    "                      DST_DATASET_DISCARDED_LABELS_TOO_SMALL_PATH\n",
    "                    )\n",
    "\n",
    "# Take all the datasets path in the source directory\n",
    "datasets_dir = [os.path.join(SRC_DATASETS_PATH, ds_name) for ds_name in os.listdir(SRC_DATASETS_PATH) if os.path.isdir(os.path.join(SRC_DATASETS_PATH, ds_name))]\n",
    "\n",
    "# Struc used to build a new yaml class mapping for the new Dataset compliant with the YOLOv8 format\n",
    "new_config_file = {}\n",
    "\n",
    "for dataset_dir in datasets_dir:\n",
    "    \n",
    "    # TODO:: Remove this if statement\n",
    "    if \"Discarded\" in dataset_dir: continue\n",
    "    \n",
    "    # Check the existence of the configuration file\n",
    "    if os.path.exists(os.path.join(dataset_dir, \"data.yaml\")):\n",
    "    # Get the configuration file of the current analyzed dataset\n",
    "        dataset_config_path = os.path.join(dataset_dir, \"data.yaml\")\n",
    "        \n",
    "        with open(dataset_config_path, \"r\") as config_stream:\n",
    "            config_label_map: list = yaml.safe_load(config_stream)[\"names\"]\n",
    "            idx_offset = max(new_config_file.keys(), default=-1)\n",
    "    else: \n",
    "        print(f\"Configuration file Not found for dataset: {dataset_dir}\")\n",
    "        \n",
    "    \n",
    "    for dataset_split in [\"train\", \"valid\", \"test\"]:\n",
    "\n",
    "        # Check if the dataset split path exist\n",
    "        if not os.path.exists(os.path.join(dataset_dir, dataset_split, \"images\")):\n",
    "            continue\n",
    "\n",
    "        print(f\"Serving {dataset_dir}: {dataset_split}\")\n",
    "\n",
    "        # Defining the considered dataset directory path\n",
    "        src_dataset_images_path = os.path.join(dataset_dir, dataset_split, \"images\")\n",
    "        src_dataset_labels_path = os.path.join(dataset_dir, dataset_split, \"labels\")\n",
    "\n",
    "        #===========================================================================\n",
    "        # Creating a set we eliminate all the duplicates from the list.\n",
    "        all_images_files = [im for im in os.listdir(src_dataset_images_path) if os.path.isfile(os.path.join(src_dataset_images_path, im))]\n",
    "        all_labels_files_list = [lb for lb in os.listdir(src_dataset_labels_path) if os.path.isfile(os.path.join(src_dataset_labels_path, lb))]\n",
    "        all_labels_files_set = set(all_labels_files_list)\n",
    "\n",
    "        # Check that there are no duplicated or missing labels file\n",
    "        assert len(all_labels_files_set) == len(all_labels_files_list)\n",
    "        #===========================================================================\n",
    "\n",
    "\n",
    "        for image_name in tqdm.tqdm(all_images_files):\n",
    "            \n",
    "            # Check if the image has its correspondent match in the label files\n",
    "            label_name = os.path.splitext(image_name)[0] + \".txt\"\n",
    "            # This check have a complexity of O(1)\n",
    "            assert label_name in all_labels_files_set\n",
    "\n",
    "                        \n",
    "            #===========================================================================\n",
    "            # TODO:: imporve the redability of this code\n",
    "            \n",
    "            # Check that the label file is not empty\n",
    "            if not isLabelFileEmpty(os.path.join(src_dataset_labels_path, label_name)):\n",
    "\n",
    "                # Check that the bounding box is big enough\n",
    "                if discardBBdimension(imagePath=os.path.join(src_dataset_images_path, image_name),\n",
    "                                      labelPath=os.path.join(src_dataset_labels_path, label_name)):\n",
    "                    shutil.copy(f\"{os.path.join(src_dataset_images_path, image_name)}\", f\"{DST_DATASET_DISCARDED_IMAGES_TOO_SMALL_PATH}\")\n",
    "                    shutil.copy(f\"{os.path.join(src_dataset_labels_path, label_name)}\", f\"{DST_DATASET_DISCARDED_LABELS_TOO_SMALL_PATH}\")\n",
    "                    continue\n",
    "\n",
    "                # Copy the image and label in the new folder\n",
    "                shutil.copy(os.path.join(src_dataset_images_path, image_name), DST_DATASET_IMAGES_PATH)\n",
    "                shutil.copy(os.path.join(src_dataset_labels_path, label_name), DST_DATASET_LABELS_PATH)\n",
    "\n",
    "                # Change the label file to be consistent with the new label mapping\n",
    "                with open(os.path.join(DST_DATASET_LABELS_PATH, label_name), \"r\") as label_fin:\n",
    "                    file_content = \"\"\n",
    "                    for line_number, line in enumerate(label_fin):\n",
    "                        # Eliminate all the newLine and split by space\n",
    "                        line = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                        # The first value is the label value.\n",
    "                        # Update of the value considering the new mapping\n",
    "                        line[0] = str(int(line[0]) + idx_offset + 1)\n",
    "                        # Re-join the line content\n",
    "                        line = \" \".join(line)\n",
    "                        # If the file have only one line we do not append a newLine.\n",
    "                        # In practice we are rebuilding the file but with all the lines in the reverse order\n",
    "                        file_content = line if line_number == 0 else file_content + \"\\n\" + line \n",
    "                \n",
    "                # Rewrite the file with the updated content\n",
    "                with open(os.path.join(DST_DATASET_LABELS_PATH, label_name), \"w\") as label_fin:\n",
    "                    label_fin.write(file_content)\n",
    "\n",
    "            else:\n",
    "                shutil.copy(f\"{os.path.join(src_dataset_images_path, image_name)}\", f\"{DST_DATASET_DISCARDED_IMAGES_NO_LABELS_PATH}\")\n",
    "                shutil.copy(f\"{os.path.join(src_dataset_labels_path, label_name)}\", f\"{DST_DATASET_DISCARDED_LABELS_NO_LABELS_PATH}\")\n",
    "            #===========================================================================\n",
    "        print(f\"Done with {dataset_dir}: {dataset_split}\")\n",
    "        \n",
    "            \n",
    "                \n",
    "    # Update the new config file for the merged dataset\n",
    "    for i, name in enumerate(config_label_map, start=idx_offset + 1):\n",
    "        new_config_file[i] = name  \n",
    "    print(new_config_file)\n",
    "\n",
    "\n",
    "#===========================================================================\n",
    "# Write the updated configuration file\n",
    "config_file = {\n",
    "    \"train\": \"../train/images\",\n",
    "    \"val\": \"../valid/images\",\n",
    "    \"test\": \"../test/images\",\n",
    "\n",
    "    \"nc\": len(new_config_file),\n",
    "    \"names\": new_config_file,\n",
    "}\n",
    "\n",
    "with open(os.path.join(DST_DATASET_PATH, \"data.yaml\"), \"w\") as file: \n",
    "    yaml.dump(config_file, file)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounding Dino Automatic Label un-labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill_grounding_dino import GroundingDINO\n",
    "\n",
    "from autodistill.detection import CaptionOntology\n",
    "from autodistill.helpers import sync_with_roboflow\n",
    "import roboflow\n",
    "\n",
    "\n",
    "UNLABELED_IMAGES_PATH = \"GigaDataset\\\\discarded\\\\noLabels\\\\images\"\n",
    "LABELED_IMAGES_PATH = \"GigaDataset\\\\re-labeled\\\\images_labeled\"\n",
    "\n",
    "# TAKS = \"detection\"\n",
    "TAKS = \"segmentation\" \n",
    "\n",
    "CAPTION_ONTOLOGY = {\n",
    "    \"black robot with colored light\": \"robot\"\n",
    "}\n",
    "\n",
    "BOX_THRESHOLD = 0.5\n",
    "TEXT_THRESHOLD = 0.70\n",
    "\n",
    "classes = {i: label for i, label in enumerate(CAPTION_ONTOLOGY.values())}\n",
    "\n",
    "mode_f = GroundedSAM if TAKS == \"segmentation\" else GroundingDINO\n",
    "model = mode_f(\n",
    "    ontology=CaptionOntology(CAPTION_ONTOLOGY),\n",
    "    box_threshold=BOX_THRESHOLD,\n",
    "    text_threshold=TEXT_THRESHOLD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To autolabel directly on Roboflow (VERY SLOW !!!!!!!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roboflow.login(force=True)\n",
    "\n",
    "# sync_with_roboflow(\n",
    "#     workspace_id=\"yBkLwcSpuygMbFFpUWPp6nvZwbo1\",\n",
    "#     workspace_url=\"ilchrees\",\n",
    "#     project_id = \"robotsegment\",\n",
    "#     batch_id = \"R1kRVKKcVSdMDRYDtNG9\",\n",
    "#     model = model\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grounding Dino on random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code before starting labeling all images together to check if the process does not have bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import supervision as sv\n",
    "import random\n",
    "\n",
    "# From all the images choose one randomly\n",
    "IMAGE_NAME = random.choice([im_name for im_name in os.listdir(UNLABELED_IMAGES_PATH) if os.path.isfile(os.path.join(UNLABELED_IMAGES_PATH, im_name))])\n",
    "\n",
    "image_path = os.path.join(UNLABELED_IMAGES_PATH, IMAGE_NAME)\n",
    "\n",
    "predictions = model.predict(image_path)\n",
    "\n",
    "print(f\"Prediction struct length: {len(predictions)}\")\n",
    "labels = [f\"{classes[class_id]} {confidence:0.2f}\" for _, _, confidence, class_id, _ , _ in predictions]\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "annotator = sv.BoxAnnotator()\n",
    "annotated_image = annotator.annotate(scene=image, detections=predictions, labels=labels)\n",
    "\n",
    "sv.plot_image(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label The Unlabeled images using Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.label(input_folder=UNLABELED_IMAGES_PATH, output_folder=LABELED_IMAGES_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roboto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
